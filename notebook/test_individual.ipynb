{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Set working directory\"\n",
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from math import exp, log\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "from socpd.objects import Object\n",
    "from socpd.agent import Agent\n",
    "\n",
    "from socpd.hypothesis_nw import PARAMS_INDIVIDUAL, \\\n",
    "    PARAMS_IPF_WEIGHTS\n",
    "\"\"\"\n",
    "Agentpy Agent Module\n",
    "Content: Agent Classes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"comma module\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class Individual(Agent):\n",
    "\n",
    "    def setup(self):\n",
    "        # linking with model #--> Agent\n",
    "        \n",
    "        self.random = self.model.random\n",
    "        #self.status_quo = self.model.status_quo\n",
    "        # Network method\n",
    "        self.network = self.model.network\n",
    "        \n",
    "        #local variable\n",
    "        self.status = False\n",
    "        self.moving= False\n",
    "        self.features = None\n",
    "        self.influencing_profile : pd.DataFrame\n",
    "        #self.influenced_score :  pd.Series\n",
    "        #self.move_to_pos = None\n",
    "        self.share_similar_diet :float = .0\n",
    "\n",
    "        #call-out attribute from hypothesis/model \n",
    "        #self.env_beta = self.model._env_beta\n",
    "        self.status_var = self.model.status_var\n",
    "  \n",
    "            # Hypothesis's params:\n",
    "        self.params_self  = self.model.rules['actions_to_self']\n",
    "        self.params_nw = self.model.rules['actions_to_nw']\n",
    "            # for masking actions\n",
    "        homo_neg  = self.model.homo_neg # neg_nw_actions\n",
    "        homo_pos = self.model.homo_pos # pos_nw_actions\n",
    "        \n",
    "        \n",
    "        #____________________ to Network _______________________\n",
    "        \n",
    "        # to update influencing_profile by agent's status\n",
    "        self.masked_neg = np.isin(self.params_nw.index, homo_neg)\n",
    "        self.masked_pos = np.isin(self.params_nw.index, homo_pos)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_p(sum_betas:float):\n",
    "        '''logit reversed'''\n",
    "        return exp(sum_betas)/(1+exp(sum_betas))\n",
    "    \n",
    "    #_________________________________________________________________\n",
    "    # SETUP agent's status at t = 0\n",
    "    def get_status_step0(self):\n",
    "        \"\"\"\n",
    "        Get agent status at t = 0 \n",
    "        Returns: None -> change agents' status to True if status_var_yes = 1\n",
    "        \"\"\"\n",
    "        fs = self.features\n",
    "        self.status = fs[f'{self.status_var}_yes'] == 1 \n",
    "\n",
    "    #________________________________________________________________\n",
    "    # Set influencing profile by statu\n",
    "    def update_influencing_profile_by_status(self):\n",
    "        '''Set feature-customized influencing-profile of each agents     \n",
    "        Required: \n",
    "            Hypothesis rules (param_nw)\n",
    "            updated status (from t0)\n",
    "            features\n",
    "        Return: None\n",
    "            Update influencing_profile by status \n",
    "            actions against agent's status will be zero out\n",
    "        '''\n",
    "        params_nw = np.array(self.params_nw)\n",
    "        features = np.array(self.features)\n",
    "        influencing_profile = np.multiply(params_nw,features)\n",
    "        # zero-out the influence doesn't match agent's status\n",
    "        if self.status:\n",
    "            influencing_profile[self.masked_neg,:] = 0.0\n",
    "        else:\n",
    "            influencing_profile[self.masked_pos,:] = 0.0\n",
    "\n",
    "        # set agent's influencing profile\n",
    "        self.influencing_profile = influencing_profile\n",
    "    \n",
    "        \n",
    "    def update_agent_combined(self):\n",
    "        \n",
    "        '''Finalize all score combination\n",
    "            - update segregration\n",
    "            - update self.status'''\n",
    "            \n",
    "        \"\"\"Get scores for all actions to self\"\"\"\n",
    "        #status_quo = self.model.status_quo    \n",
    "        _score_self = np.array(self.params_self).dot(np.array(self.features))\n",
    "        _score_self_adopt = np.sum(_score_self)\n",
    "        _score_adopt = _score_self_adopt #+ self.env_beta + status_quo\n",
    "        print(f'self score {_score_adopt}')\n",
    "        \"\"\" Get scores from neighbor by shared similarity\"\"\"\n",
    "        ln = self.network.graph.degree(self.network.positions[self])\n",
    "        if ln > 0: \n",
    "            neighbors = self.network.neighbors(self)\n",
    "\n",
    "            # Update segregration ( p of neighbors with same status ______________________________________________________\n",
    "            #similar = len([n for n in neighbors if n.status == self.status])\n",
    "            #self.share_similar_diet = similar / ln    \n",
    "            # Get scores from neighbor' actions ________________________________________________________\n",
    "                # get sum scores of influence from neighbors by homophily\n",
    "            neigh_pfs = np.array([n.influencing_profile for n in neighbors])\n",
    "            _neigh_scores = neigh_pfs.dot(np.array(self.features)) \n",
    "            _score_nw_adopt =  np.sum(_neigh_scores) \n",
    "            _score_adopt += _score_nw_adopt \n",
    "            \n",
    "            pos_n = len([n for n in neighbors if n.status == True])\n",
    "            print(ln)\n",
    "            print(pos_n)\n",
    "            \n",
    "            if pos_n == ln or pos_n ==0:\n",
    "                _score_adopt * ln\n",
    "            else:\n",
    "                _score_adopt += log(pos_n/(ln-pos_n))\n",
    "        \n",
    "        print(f'final score{_score_adopt}')\n",
    "        \"\"\" UPDATE self.status \"\"\"\n",
    "        prob_adopt = self.get_p(_score_adopt)\n",
    "        self.status = self.random.random() <= prob_adopt\n",
    "    \n",
    "\n",
    "    '''STEP 4 : taking action following  moving and status'''\n",
    "    #def find_new_friends(self):\n",
    "    #    self.grid.move_to(self, self.move_to_pos)\n",
    "        \n",
    "    def change_agent_features_by_status(self):\n",
    "        if self.status:\n",
    "            self.features[self.features.index.str.contains(pat = f'{self.status_var}')] = [0,1]\n",
    "        else:\n",
    "            self.features[self.features.index.str.contains(pat = f'{self.status_var}')] = [1,0]    \n",
    "\n",
    "\n",
    "\n",
    "class Populating(Object):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(model)\n",
    "        self.nprandom = model.nprandom\n",
    "        self.all_possible_features = self.model.all_possible_features\n",
    "           \n",
    "    def sampling_from_ipf(self, dir_params: str, pop:int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Sample from IPF distribution saved\n",
    "        as `weights.csv` in the parameters folder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pop (int): size of data sample\n",
    "        dir_params (str): path to the parameters folder\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample (pandas.dataFrame): dataframe containing the sampling\n",
    "        \"\"\"\n",
    "        fpath_weights = os.path.join(dir_params, PARAMS_IPF_WEIGHTS)\n",
    "        assert os.path.isfile(fpath_weights)\n",
    "        df_weights = pd.read_csv(fpath_weights, sep=\",\", index_col=0)\n",
    "        weights = df_weights[\"weight\"] / df_weights[\"weight\"].sum()\n",
    "        indices = df_weights.index\n",
    "        \n",
    "\n",
    "        sample_indices = self.nprandom.choice(indices, pop, p=weights)\n",
    "        sample = df_weights.loc[sample_indices].drop([\"weight\"], axis=1)\n",
    "        sample = sample.reset_index(drop=True)\n",
    "        return sample\n",
    "\n",
    "    def populate_ipf(self, dir_params: str, pop:int) -> List:\n",
    "        \"\"\"\n",
    "        Create a population of individual agents\n",
    "        with the given weights obtained via IPF\n",
    "\n",
    "        Args:\n",
    "            pop (int): size of data sample.\n",
    "            dir_params (str): path to parameters folder.\n",
    "\n",
    "        Returns:\n",
    "            List[Individual]: A list containing instances of\n",
    "            the individual class, each representing an\n",
    "            agent with specific features.\n",
    "        \"\"\"\n",
    "        _features = pd.DataFrame()\n",
    "\n",
    "        sample = self.sampling_from_ipf(dir_params, pop)\n",
    "\n",
    "        # one-hot encoding\n",
    "        encoded_columns = pd.get_dummies(sample).reindex(\n",
    "            columns=self.all_possible_features,\n",
    "            fill_value=0\n",
    "        )\n",
    "        _features = pd.concat([_features, encoded_columns], axis=1)\n",
    "        #________________added\n",
    "        cols = sorted(_features.columns)\n",
    "        _features = _features[cols]\n",
    "\n",
    "        # Add 'baseline' column filled with ones if this is not present yet\n",
    "        if 'baseline' not in _features.columns:\n",
    "            _features.insert(0, \"baseline\", 1)\n",
    "\n",
    "        return [_features.iloc[i] for i in\n",
    "                tqdm(range(pop), desc=\"Populating individuals\", unit=\"i\")]\n",
    "\n",
    "    def populate_simple(self, dir_params: str, pop:int) -> List:\n",
    "        \"\"\"\n",
    "        Create a population of individual agents\n",
    "        with the given feature parameters.\n",
    "\n",
    "        Args:\n",
    "            pop (int): population size, i.e., number of agents.\n",
    "            dir_params (str): dir to the folder containing\n",
    "            feature parameter file.\n",
    "            #from_scratch (bool, optional): flag of creating hypothesis\n",
    "            from scratch or reading from files. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            list[Individual]: a list of Individual agents\n",
    "        \"\"\"\n",
    "        assert pop > 0, 'Size must be positive!'\n",
    "        assert isinstance(pop, int), 'Pop - population size must be integer!'\n",
    "        assert os.path.isdir(dir_params), \"Given folder doesn't exist!\"\n",
    "\n",
    "        fpath_params_individual = os.path.join(dir_params, PARAMS_INDIVIDUAL)\n",
    "        with open(fpath_params_individual) as f:\n",
    "            features = json.load(f)\n",
    "        features = {k.lower():v for k, v in features.items()}\n",
    "\n",
    "        _features = pd.DataFrame()\n",
    "        for feature, distribution in features.items():\n",
    "            _features[feature] = self.nprandom.choice(\n",
    "                distribution[0], pop, p=distribution[1]\n",
    "            )\n",
    "\n",
    "            # Define all possible columns (including those not in the sample)\n",
    "            # When the sample size is too small,\n",
    "            # this doesn't cover all categories,\n",
    "            # the resulting DataFrame thus lacks those columns.\n",
    "            # To solve the issue, we ensure all possible categories are present\n",
    "            # when creating the dummy variables\n",
    "\n",
    "        # one-hot encoding\n",
    "        categorical_cols = _features.select_dtypes(include=['object'])\n",
    "        encoded_cols = pd.get_dummies(categorical_cols).reindex(\n",
    "            columns=self.all_possible_features,\n",
    "            fill_value=0\n",
    "        )\n",
    "        _features.drop(categorical_cols.columns, axis=1, inplace=True)\n",
    "        _features = pd.concat([_features, encoded_cols], axis=1)\n",
    "        #________________added\n",
    "        cols = sorted(_features.columns)\n",
    "        _features = _features[cols]\n",
    "\n",
    "        # Add 'baseline' column filled with ones\n",
    "        _features.insert(0, \"baseline\", 1)\n",
    "\n",
    "        return [_features.iloc[i] for i in\n",
    "                tqdm(range(pop), desc=\"Populating individuals\", unit=\"i\")]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "import networkx as nx\n",
    "\n",
    "#agentpy\n",
    "from socpd.model import Model\n",
    "from socpd.network import Network\n",
    "from socpd.sequences import AgentList\n",
    "\n",
    "from socpd.hypothesis_nw import Hypothesis\n",
    " \n",
    "\n",
    "class SocPD(Model, Hypothesis):  \n",
    "    def setup(self) :\n",
    "               \n",
    "        # Set-up Hypothesis \n",
    "        Hypo_dict = self.p.Hypothesis_settings #_____________ added\n",
    "        Hypothesis.validate_n_read_hypotheses(Hypo_dict)#__________________added\n",
    "\n",
    "        #call-out hypothesis on actions\n",
    "        self._dir_params = Hypothesis.dir_params\n",
    "        self.status_var = Hypothesis.status_var\n",
    "        self.all_possible_features = Hypothesis.all_possible_features\n",
    "  \n",
    "        self.homo_neg = Hypothesis.homo_neg\n",
    "        self.homo_pos = Hypothesis.homo_pos    \n",
    "        self.rules = Hypothesis.rules\n",
    "        \n",
    "        # Private attributes_________________________________________________________________\n",
    "            # status_quo, env_beta,use_ipf, pop, \n",
    "        #self.status_quo : float = .0\n",
    "        if 'env_beta' in self.p:\n",
    "            self._env_beta = float(self.p['env_beta'])\n",
    "        else:\n",
    "            self._env_beta = 0.0\n",
    "        self.report(\"intervention's effect-env_beta\", self._env_beta)\n",
    "\n",
    "            # Specifying use ipf\n",
    "        #self._use_ipf : bool = None\n",
    "        if 'use_ipf' in self.p:\n",
    "            self._use_ipf = self.p['use_ipf']\n",
    "        else:\n",
    "            self._use_ipf = False \n",
    "        self.report('use_ipf', self._use_ipf)\n",
    "                \n",
    "        #----------------------------------------------#\n",
    "        #_______PREPARE NETWORK_____________________\n",
    "\n",
    "        pop = self.pop = self.p['pop']\n",
    "        m = self.p['m']\n",
    "        p = self.p['p']\n",
    "        q = self.p['q']\n",
    "        graph = nx.extended_barabasi_albert_graph(\n",
    "            n = pop, \n",
    "            m = m, # there are 1-p-q chance a new nodes can connect with m other existing nodes based on attachment preference\n",
    "            p = p, # each existing nodes has p prob to form a new links to the others with attachment preference\n",
    "            q = q, # each existing nodes has q prob to rewire one of thier existin links with attachment preference\n",
    "            seed=self.random)\n",
    "        self.network = Network(self, graph)\n",
    "        \n",
    "            # Report network's attributes\n",
    "        degree_sequence = np.array([d for _, d in graph.degree()])\n",
    "        self.report(\"Max_nw_size\", np.max(degree_sequence))\n",
    "        self.report(\"Min_nw_size\", np.min(degree_sequence))\n",
    "        self.report(\"AVG_nw_size\", np.mean(degree_sequence))\n",
    "        \n",
    "        \n",
    "        #_______GENERATE AGENTS______________________\n",
    "        self.agents = AgentList(self, pop, Individual)\n",
    "        \n",
    "            # generate features then update agent's features and status\n",
    "            # Specifying use_ipf\n",
    "        self.Populating = Populating(self)\n",
    "        if self._use_ipf:\n",
    "            _feature_iter = self.Populating.populate_ipf(self._dir_params, pop)\n",
    "        else:\n",
    "            _feature_iter = self.Populating.populate_simple(self._dir_params, pop)\n",
    "            \n",
    "            # update agent's features and status step 0\n",
    "        for i, a in enumerate(self.agents):\n",
    "            a.features = _feature_iter[i]\n",
    "            \n",
    "        self.agents.get_status_step0()\n",
    "        \n",
    "        #_______ADD AGENTS ON network_________________________\n",
    "        \n",
    "        self.network.add_agents(self.agents, self.network.nodes)\n",
    "\n",
    "  \n",
    "    def update(self): \n",
    "        positive_p = len(self.agents.select(self.agents.status==True))/self.pop\n",
    "                # Stop simulation if all are positive or negative\n",
    "        if positive_p <= 0.01 or positive_p >= 0.99:\n",
    "            self.stop()\n",
    "        \n",
    "        #self.status_quo = log(positive_p/(1-positive_p))\n",
    "\n",
    "        #record status\n",
    "        self['positive'] = positive_p\n",
    "        self.record('positive')\n",
    "        self['negative'] = 1 - self['positive']\n",
    "        self.record('negative')\n",
    "        \n",
    "        # update from t = 0 \n",
    "        self.agents.update_influencing_profile_by_status()  \n",
    "        #self.agents.update_status_quo()\n",
    "        \n",
    "        #if self.nw3D:\n",
    "        self.agents.update_agent_combined()\n",
    "        #self.unhappy = self.agents.select(self.agents.moving == True)\n",
    "        #self.unhappy.find_new_friends()\n",
    "     \n",
    " \n",
    "\n",
    "\n",
    "       \n",
    "    def step(self) : \n",
    "\n",
    "        self.agents.change_agent_features_by_status()\n",
    "        \n",
    "    #def get_segregation(self):\n",
    "        # Calculate average percentage of similar neighbors\n",
    "    #    return round(sum(self.agents.share_similar_diet) / self.pop, 2)\n",
    "\n",
    "    def end(self):\n",
    "        # Measure segregation at the end of the simulation\n",
    "        #self.report('segregation', self.get_segregation())        \n",
    "        self.report(f'Final_{self.status_var}_proportion', self.positive) \n",
    "        self.report(f'Peak_{self.status_var}_proportion', max(self.log['positive']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_params = 'parameters_demo'\n",
    "all_possible_actions=  [ 'Veg_prob',\n",
    "                          'concerns_health',\n",
    "                         'concerns_animal_welfare',\n",
    "                         'concerns_environement',\n",
    "                         'concerns_convinience',\n",
    "                         'concerns_familiarity',\n",
    "                         'concerns_taste',\n",
    "                         'concerns_price',\n",
    "                         'concerns_hunger',\n",
    "                         'homo_veg_inf',\n",
    "                         'homo_om_inf',\n",
    "\n",
    "                        ]\n",
    "\n",
    "Hypothesis_settings = { 'dir_params' : dir_params , # folder name stored actions parameters\n",
    "                        'status_var':'vegetarian',  # the variables deciding agents' status\n",
    "                        # all actions from the actions parameters\n",
    "                        'all_possible_actions':all_possible_actions,\n",
    "                        # all actions/events influencing directly on the agent\n",
    "                        'actions_to_self':['Veg_prob',\n",
    "                                            'concerns_health',\n",
    "                                            'concerns_animal_welfare',\n",
    "                                            'concerns_environement',\n",
    "                                            'concerns_convinience',\n",
    "                                            'concerns_familiarity',\n",
    "                                            'concerns_taste',\n",
    "                                            'concerns_price',\n",
    "                                            'concerns_hunger',],\n",
    "                                            \n",
    "                        # all actions/events from the agent influencing on others in thier networks\n",
    "                        'actions_to_nw':['homo_veg_inf',\n",
    "                                        'homo_om_inf',],\n",
    "\n",
    "                        # actions/events from the agents having negative effects on thier networks \n",
    "                        'actions_to_nw_neg':['homo_om_inf'],\n",
    "                        # actions/events from the agents having positive effects on thier networks \n",
    "                        'actions_to_nw_pos':['homo_veg_inf',],\n",
    "                        }\n",
    " \n",
    "\n",
    "\n",
    "parameters = {'Hypothesis_settings' : Hypothesis_settings,\n",
    "            'steps': 4,    \n",
    "            'seed' : 42,   \n",
    "            'pop' : 20, # size of the population\n",
    "            'm': 1, # Number of edges with which a new node attaches to existing nodes\n",
    "            'p' : 0, # each existing nodes has p prob to form m new link(s) to the others with attachment preference\n",
    "            'q' : .78 # each existing nodes has q prob to rewire m existing link(s) with attachment preference\n",
    "            #'use_ipf': False,   \n",
    "            } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Populating individuals: 100%|██████████| 20/20 [00:00<00:00, 7183.26i/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self score -1.7190000000000003\n",
      "5\n",
      "0\n",
      "final score-11.827000000000002\n",
      "self score 2.513\n",
      "2\n",
      "0\n",
      "final score-0.2370000000000001\n",
      "self score -0.6440000000000002\n",
      "2\n",
      "0\n",
      "final score-3.9780000000000006\n",
      "self score -1.0259999999999994\n",
      "4\n",
      "0\n",
      "final score-5.511999999999999\n",
      "self score 0.8439999999999998\n",
      "1\n",
      "0\n",
      "final score-1.7730000000000001\n",
      "self score -0.26600000000000046\n",
      "1\n",
      "0\n",
      "final score-1.9280000000000004\n",
      "self score 0.5939999999999998\n",
      "5\n",
      "0\n",
      "final score-9.701\n",
      "self score -0.2960000000000004\n",
      "2\n",
      "0\n",
      "final score-4.23\n",
      "self score -0.29899999999999893\n",
      "3\n",
      "0\n",
      "final score-6.164999999999998\n",
      "self score -3.452000000000001\n",
      "2\n",
      "0\n",
      "final score-8.386000000000001\n",
      "self score 2.716\n",
      "1\n",
      "0\n",
      "final score1.1710000000000003\n",
      "self score -2.0540000000000003\n",
      "1\n",
      "0\n",
      "final score-0.7540000000000002\n",
      "self score -1.1390000000000005\n",
      "2\n",
      "0\n",
      "final score-2.431\n",
      "self score -1.6320000000000001\n",
      "1\n",
      "0\n",
      "final score-3.5940000000000003\n",
      "self score -1.186\n",
      "2\n",
      "0\n",
      "final score-1.548\n",
      "self score 2.4739999999999993\n",
      "final score2.4739999999999993\n",
      "self score 1.7640000000000002\n",
      "1\n",
      "0\n",
      "final score0.6840000000000002\n",
      "self score -0.45399999999999985\n",
      "1\n",
      "0\n",
      "final score-1.516\n",
      "self score 2.534\n",
      "1\n",
      "0\n",
      "final score1.9339999999999997\n",
      "self score 0.8439999999999998\n",
      "1\n",
      "0\n",
      "final score-1.1630000000000003\n",
      "self score -1.7190000000000003\n",
      "5\n",
      "0\n",
      "final score-11.827000000000002\n",
      "self score -0.6869999999999997\n",
      "2\n",
      "0\n",
      "final score-4.960999999999999\n",
      "self score -0.6440000000000002\n",
      "2\n",
      "0\n",
      "final score-3.9780000000000006\n",
      "self score -1.0259999999999994\n",
      "4\n",
      "0\n",
      "final score-7.3740000000000006\n",
      "self score 0.8439999999999998\n",
      "1\n",
      "0\n",
      "final score-1.7730000000000001\n",
      "self score -0.26600000000000046\n",
      "1\n",
      "0\n",
      "final score-1.9280000000000004\n",
      "self score 0.5939999999999998\n",
      "5\n",
      "0\n",
      "final score-9.701\n",
      "self score -0.2960000000000004\n",
      "2\n",
      "0\n",
      "final score-4.23\n",
      "self score -0.29899999999999893\n",
      "3\n",
      "0\n",
      "final score-6.164999999999998\n",
      "self score -3.452000000000001\n",
      "2\n",
      "0\n",
      "final score-8.386000000000001\n",
      "self score 2.716\n",
      "1\n",
      "0\n",
      "final score1.1710000000000003\n",
      "self score -2.0540000000000003\n",
      "1\n",
      "0\n",
      "final score-4.016\n",
      "self score -1.1390000000000005\n",
      "2\n",
      "0\n",
      "final score-6.593\n",
      "self score -1.6320000000000001\n",
      "1\n",
      "0\n",
      "final score-3.5940000000000003\n",
      "self score -1.186\n",
      "2\n",
      "0\n",
      "final score1.604\n",
      "self score 2.4739999999999993\n",
      "final score2.4739999999999993\n",
      "self score 1.7640000000000002\n",
      "1\n",
      "0\n",
      "final score0.6840000000000002\n",
      "self score -0.45399999999999985\n",
      "1\n",
      "0\n",
      "final score-1.516\n",
      "self score -0.6660000000000005\n",
      "1\n",
      "0\n",
      "final score-2.0280000000000005\n",
      "self score 4.044\n",
      "1\n",
      "0\n",
      "final score2.7989999999999995\n",
      "Completed: 1 stepsself score -1.7190000000000003\n",
      "5\n",
      "0\n",
      "final score-14.649000000000001\n",
      "self score -0.6869999999999997\n",
      "2\n",
      "0\n",
      "final score-4.960999999999999\n",
      "self score -0.6440000000000002\n",
      "2\n",
      "0\n",
      "final score-3.9780000000000006\n",
      "self score -1.0259999999999994\n",
      "4\n",
      "0\n",
      "final score-4.149999999999999\n",
      "self score 0.8439999999999998\n",
      "1\n",
      "0\n",
      "final score-1.7730000000000001\n",
      "self score -0.26600000000000046\n",
      "1\n",
      "0\n",
      "final score-1.9280000000000004\n",
      "self score 0.5939999999999998\n",
      "5\n",
      "0\n",
      "final score-9.701\n",
      "self score -0.2960000000000004\n",
      "2\n",
      "0\n",
      "final score-4.23\n",
      "self score -0.29899999999999893\n",
      "3\n",
      "0\n",
      "final score-6.164999999999998\n",
      "self score -3.452000000000001\n",
      "2\n",
      "0\n",
      "final score-8.386000000000001\n",
      "self score -0.48399999999999976\n",
      "1\n",
      "0\n",
      "final score1.1610000000000003\n",
      "self score -2.0540000000000003\n",
      "1\n",
      "0\n",
      "final score-4.016\n",
      "self score -1.1390000000000005\n",
      "2\n",
      "0\n",
      "final score-6.593\n",
      "self score -1.6320000000000001\n",
      "1\n",
      "0\n",
      "final score-3.5940000000000003\n",
      "self score 2.0139999999999993\n",
      "2\n",
      "0\n",
      "final score2.345999999999999\n",
      "self score 2.4739999999999993\n",
      "final score2.4739999999999993\n",
      "self score -1.4359999999999995\n",
      "1\n",
      "0\n",
      "final score-3.2779999999999996\n",
      "self score 2.746\n",
      "1\n",
      "0\n",
      "final score2.446\n",
      "self score 2.534\n",
      "1\n",
      "0\n",
      "final score1.9339999999999997\n",
      "self score 4.044\n",
      "1\n",
      "0\n",
      "final score5.920999999999999\n",
      "Completed: 2 stepsself score -1.7190000000000003\n",
      "5\n",
      "0\n",
      "final score-14.649000000000001\n",
      "self score -0.6869999999999997\n",
      "2\n",
      "0\n",
      "final score-4.960999999999999\n",
      "self score -0.6440000000000002\n",
      "2\n",
      "0\n",
      "final score-3.9780000000000006\n",
      "self score -1.0259999999999994\n",
      "4\n",
      "0\n",
      "final score-4.149999999999999\n",
      "self score 0.8439999999999998\n",
      "1\n",
      "0\n",
      "final score-1.7730000000000001\n",
      "self score -0.26600000000000046\n",
      "1\n",
      "0\n",
      "final score-1.9280000000000004\n",
      "self score 0.5939999999999998\n",
      "5\n",
      "0\n",
      "final score-6.539000000000001\n",
      "self score -0.2960000000000004\n",
      "2\n",
      "0\n",
      "final score-4.23\n",
      "self score -0.29899999999999893\n",
      "3\n",
      "0\n",
      "final score-6.164999999999998\n",
      "self score -3.452000000000001\n",
      "2\n",
      "0\n",
      "final score-8.386000000000001\n",
      "self score 2.716\n",
      "1\n",
      "0\n",
      "final score5.093\n",
      "self score -2.0540000000000003\n",
      "1\n",
      "0\n",
      "final score-4.016\n",
      "self score -1.1390000000000005\n",
      "2\n",
      "0\n",
      "final score-6.593\n",
      "self score 1.568\n",
      "1\n",
      "0\n",
      "final score0.3680000000000001\n",
      "self score 2.0139999999999993\n",
      "2\n",
      "0\n",
      "final score6.267999999999999\n",
      "self score 2.4739999999999993\n",
      "final score2.4739999999999993\n",
      "self score -1.4359999999999995\n",
      "1\n",
      "0\n",
      "final score-3.2779999999999996\n",
      "self score 2.746\n",
      "1\n",
      "0\n",
      "final score2.446\n",
      "self score 2.534\n",
      "1\n",
      "0\n",
      "final score1.9339999999999997\n",
      "self score 4.044\n",
      "1\n",
      "0\n",
      "final score5.920999999999999\n",
      "Completed: 3 stepsself score -1.7190000000000003\n",
      "5\n",
      "0\n",
      "final score-14.649000000000001\n",
      "self score -0.6869999999999997\n",
      "2\n",
      "0\n",
      "final score-4.960999999999999\n",
      "self score -0.6440000000000002\n",
      "2\n",
      "0\n",
      "final score-3.9780000000000006\n",
      "self score -1.0259999999999994\n",
      "4\n",
      "0\n",
      "final score-6.011999999999999\n",
      "self score 0.8439999999999998\n",
      "1\n",
      "0\n",
      "final score-1.7730000000000001\n",
      "self score -0.26600000000000046\n",
      "1\n",
      "0\n",
      "final score-1.9280000000000004\n",
      "self score 0.5939999999999998\n",
      "5\n",
      "0\n",
      "final score-9.701\n",
      "self score -0.2960000000000004\n",
      "2\n",
      "0\n",
      "final score-4.23\n",
      "self score -0.29899999999999893\n",
      "3\n",
      "0\n",
      "final score-6.164999999999998\n",
      "self score -3.452000000000001\n",
      "2\n",
      "0\n",
      "final score-8.386000000000001\n",
      "self score 2.716\n",
      "1\n",
      "0\n",
      "final score5.093\n",
      "self score -2.0540000000000003\n",
      "1\n",
      "0\n",
      "final score-4.016\n",
      "self score -1.1390000000000005\n",
      "2\n",
      "0\n",
      "final score-6.593\n",
      "self score -1.6320000000000001\n",
      "1\n",
      "0\n",
      "final score-3.5940000000000003\n",
      "self score 2.0139999999999993\n",
      "2\n",
      "0\n",
      "final score6.267999999999999\n",
      "self score 2.4739999999999993\n",
      "final score2.4739999999999993\n",
      "self score -1.4359999999999995\n",
      "1\n",
      "0\n",
      "final score-3.2779999999999996\n",
      "self score 2.746\n",
      "1\n",
      "0\n",
      "final score2.446\n",
      "self score -0.6660000000000005\n",
      "1\n",
      "0\n",
      "final score-2.0280000000000005\n",
      "self score 4.044\n",
      "1\n",
      "0\n",
      "final score5.920999999999999\n",
      "Completed: 4 steps\n",
      "Run time: 0:00:00.064018\n",
      "Simulation finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = SocPD(parameters)\n",
    "result= model.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
